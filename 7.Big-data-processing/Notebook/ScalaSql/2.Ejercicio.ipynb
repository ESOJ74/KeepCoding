{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03f9ce53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.nio.file.Files\n",
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4ce65384\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file.Files\n",
    "import org.apache.spark.sql.SparkSession\n",
    "//import spark.implicits._\n",
    "//import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Spark SQL KeepCoding Base\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "294538b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvDF: org.apache.spark.sql.DataFrame = [nombre: string, apellido: string ... 2 more fields]\n",
       "jsonDF: org.apache.spark.sql.DataFrame = [apellido: string, clase: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1. Leemos ambos datasources cada uno con su formato correspondiente.\n",
    "val csvDF = spark\n",
    "      .read\n",
    "      .format(\"csv\")\n",
    "      .option(\"sep\", \",\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(\"Data/sample.csv\")\n",
    "\n",
    "val jsonDF = spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c473cacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fullDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [apellido: string, clase: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 2. Unificamos los datos CSV & JSON, usando la función unionByName(...)\n",
    "val fullDF = jsonDF.unionByName(csvDF).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3140edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|nombre|apellido|nota_media|\n",
      "+------+--------+----------+\n",
      "|  sara|  garcia|       9.0|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 3. ¿Qué alumno tiene mejor nota media del instituto?\n",
    "\n",
    "fullDF\n",
    "      .select($\"nombre\", $\"apellido\", $\"nota\")\n",
    "      .groupBy($\"nombre\", $\"apellido\")\n",
    "      .agg(avg($\"nota\").alias(\"nota_media\"))\n",
    "      .sort($\"nota_media\".desc)\n",
    "      .limit(1)\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47ad208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| clase|nota_media|\n",
      "+------+----------+\n",
      "|ingles|       6.6|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 4. ¿Qué clase tiene las peores notas?\n",
    "\n",
    "fullDF\n",
    "      .select($\"clase\", $\"nota\")\n",
    "      .groupBy($\"clase\")\n",
    "      .agg(avg($\"nota\").as(\"nota_media\"))\n",
    "      .sort($\"nota_media\".asc)\n",
    "      .limit(1)\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff075408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------+----+-----------+\n",
      "|apellido|      clase|nombre|nota|      curso|\n",
      "+--------+-----------+------+----+-----------+\n",
      "|  ferrer|   historia|  juan|  10|humanidades|\n",
      "|  ferrer|  filosofia|  juan|   8|humanidades|\n",
      "|  ferrer|     ingles|  juan|   7|humanidades|\n",
      "|  garcia|matematicas|  sara|   9|   ciencias|\n",
      "|  garcia| tecnologia|  sara|  10|   ciencias|\n",
      "|   gomez|   historia|  juan|   6|humanidades|\n",
      "|   gomez|  filosofia|  juan|   8|humanidades|\n",
      "|   gomez|     ingles|  juan|   5|humanidades|\n",
      "|  garcia|     ingles|  sara|   8|   ciencias|\n",
      "|   gomez|matematicas|  pepe|   8|   ciencias|\n",
      "|   gomez| tecnologia|  pepe|   7|   ciencias|\n",
      "|   gomez|     ingles|  pepe|   7|   ciencias|\n",
      "| fuentes|  filosofia| lucia|   7|humanidades|\n",
      "| fuentes|   historia| lucia|   8|humanidades|\n",
      "| fuentes|     ingles| lucia|   6|humanidades|\n",
      "+--------+-----------+------+----+-----------+\n",
      "\n",
      "Result write into Resultados/exercise2_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cursoDF: org.apache.spark.sql.DataFrame = [nombre: string, apellido: string ... 1 more field]\n",
       "resultDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [apellido: string, clase: string ... 3 more fields]\n",
       "outputDir: String = Resultados/exercise2_output\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 5. Añade el tipo de curso a cada alumno 'ciencias'/'humanidades'\n",
    "    //    dentro de la columna 'curso', escribe los resultados en formato JSON\n",
    "    //    en un directorio temporal:\n",
    "    //\n",
    "    //    val outputDir = s\"\"\"${Files.createTempDirectory(\"spark\").toFile.getAbsolutePath}/results\"\"\n",
    "\n",
    "val cursoDF = fullDF\n",
    "      .select($\"nombre\", $\"apellido\", $\"clase\")\n",
    "      .groupBy($\"nombre\", $\"apellido\")\n",
    "      .agg(collect_list($\"clase\").as(\"clases\"))\n",
    "      .withColumn(\"curso\",\n",
    "        when(\n",
    "          $\"clases\" === array(Seq(\"matematicas\", \"tecnologia\", \"ingles\").map(lit): _*),\n",
    "          \"ciencias\"\n",
    "        ).otherwise(\"humanidades\")\n",
    "      )\n",
    "      .select($\"nombre\", $\"apellido\", $\"curso\")\n",
    "\n",
    "    val resultDF = fullDF.as(\"a\")\n",
    "      .join(cursoDF.as(\"b\"), $\"a.nombre\" === $\"b.nombre\" && $\"a.apellido\" === $\"b.apellido\")\n",
    "      .select((fullDF.columns.map(name => col(s\"a.$name\").as(name)) :+ col(\"b.curso\").as(\"curso\")): _*)\n",
    "      .cache()\n",
    "\n",
    "    //val outputDir = s\"\"\"${Files.createTempDirectory(\"spark\").toFile.getAbsolutePath}/results\"\"\"\n",
    "    val outputDir = \"Resultados/exercise2_output\"\n",
    "    resultDF.show()\n",
    "    resultDF\n",
    "      .write\n",
    "      .format(\"json\")\n",
    "      .save(outputDir)\n",
    "\n",
    "    println(s\"Result write into $outputDir\")\n",
    " //   spark.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d6913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
