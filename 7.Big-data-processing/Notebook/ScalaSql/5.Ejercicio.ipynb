{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c076ef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.11:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1634625573258)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types.{IntegerType, LongType, StructType, TimestampType}\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.parquet.format.IntType\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, StructType, TimestampType}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.parquet.format.IntType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fcf988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3aeaae24\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Spark SQL KeepCoding Base\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadeca57",
   "metadata": {},
   "source": [
    "## API Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bbae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+-----------+--------+\n",
      "|timestamp                                 |sensor_id|temperature|humidity|\n",
      "+------------------------------------------+---------+-----------+--------+\n",
      "|{2020-09-10 19:22:00, 2020-09-10 19:23:00}|1        |28.67      |63.67   |\n",
      "|{2020-09-10 19:23:00, 2020-09-10 19:24:00}|1        |27.9       |67.7    |\n",
      "|{2020-09-10 19:24:00, 2020-09-10 19:25:00}|1        |28.2       |62.0    |\n",
      "|{2020-09-10 19:22:00, 2020-09-10 19:23:00}|2        |29.13      |62.13   |\n",
      "|{2020-09-10 19:23:00, 2020-09-10 19:24:00}|2        |28.2       |68.5    |\n",
      "|{2020-09-10 19:24:00, 2020-09-10 19:25:00}|2        |28.11      |69.33   |\n",
      "|{2020-09-10 19:22:00, 2020-09-10 19:23:00}|3        |27.78      |69.89   |\n",
      "|{2020-09-10 19:23:00, 2020-09-10 19:24:00}|3        |29.55      |65.45   |\n",
      "|{2020-09-10 19:24:00, 2020-09-10 19:25:00}|3        |28.64      |68.14   |\n",
      "+------------------------------------------+---------+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataframeAPI_1: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframeAPI_1(spark: SparkSession): Unit = {\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      //.withColumn(\"timestamp\", ($\"timestamp\" / 60).cast(IntegerType) * 60)\n",
    "      .withColumn(\"timestamp\", window($\"timestamp\".cast(TimestampType), \"1 minute\"))\n",
    "      .groupBy($\"timestamp\", $\"sensor_id\")\n",
    "      .agg(round(avg($\"temperature\"), 2).as(\"temperature\"), round(avg($\"humidity\"), 2).as(\"humidity\"))\n",
    "      .sort(\"sensor_id\", \"timestamp\")\n",
    "      .show(truncate = false)   \n",
    "  }\n",
    "\n",
    "dataframeAPI_1(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5fb074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+-------------------+\n",
      "|sensor_id|temperature|humidity|timestamp          |\n",
      "+---------+-----------+--------+-------------------+\n",
      "|1        |28.67      |63.67   |2020-09-10 19:22:00|\n",
      "|1        |27.9       |67.7    |2020-09-10 19:23:00|\n",
      "|1        |28.2       |62.0    |2020-09-10 19:24:00|\n",
      "|2        |29.13      |62.13   |2020-09-10 19:22:00|\n",
      "|2        |28.2       |68.5    |2020-09-10 19:23:00|\n",
      "|2        |28.11      |69.33   |2020-09-10 19:24:00|\n",
      "|3        |27.78      |69.89   |2020-09-10 19:22:00|\n",
      "|3        |29.55      |65.45   |2020-09-10 19:23:00|\n",
      "|3        |28.64      |68.14   |2020-09-10 19:24:00|\n",
      "+---------+-----------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataframeAPI_2: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframeAPI_2(spark: SparkSession): Unit = {\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .groupBy($\"sensor_id\", window($\"timestamp\".cast(TimestampType), \"1 minute\"))\n",
    "      .agg(round(avg($\"temperature\"), 2).as(\"temperature\"), round(avg($\"humidity\"), 2).as(\"humidity\"))\n",
    "     // .withColumn(\"timestamp\", $\"window.start\".cast(LongType))  // para poder pasarlo a json\n",
    "      .withColumn(\"timestamp\", $\"window.start\")\n",
    "      .drop($\"window\")\n",
    "      .sort(\"sensor_id\", \"timestamp\")\n",
    "      .show(truncate = false)\n",
    " }\n",
    "\n",
    "dataframeAPI_2(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e1fd3",
   "metadata": {},
   "source": [
    "## API SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575e07ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+----------+\n",
      "|sensor_id|humidity|temperature| timestamp|\n",
      "+---------+--------+-----------+----------+\n",
      "|        1|   63.67|      28.67|1599758520|\n",
      "|        1|    67.7|       27.9|1599758580|\n",
      "|        1|    62.0|       28.2|1599758640|\n",
      "|        2|   62.13|      29.13|1599758520|\n",
      "|        2|    68.5|       28.2|1599758580|\n",
      "|        2|   69.33|      28.11|1599758640|\n",
      "|        3|   69.89|      27.78|1599758520|\n",
      "|        3|   65.45|      29.55|1599758580|\n",
      "|        3|   68.14|      28.64|1599758640|\n",
      "+---------+--------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlAPI: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sqlAPI(spark: SparkSession) = {\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .createOrReplaceTempView(\"sensor_data_view\")\n",
    "\n",
    "    spark.sql(\n",
    "      \"\"\"\n",
    "        |SELECT sensor_id,\n",
    "        |       ROUND(AVG(humidity), 2) AS humidity,\n",
    "        |       ROUND(AVG(temperature), 2) AS temperature,\n",
    "        |       CAST(window.start AS INT) AS timestamp\n",
    "        |FROM sensor_data_view\n",
    "        |GROUP BY sensor_id, window(CAST(timestamp AS TIMESTAMP), \"1 minute\")\n",
    "        |ORDER BY sensor_id, timestamp ASC\n",
    "      \"\"\".stripMargin\n",
    "    ).show()\n",
    "  }\n",
    "\n",
    "sqlAPI(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62c26b",
   "metadata": {},
   "source": [
    "## API Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa548b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.NullPointerException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.NullPointerException",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.lang.reflect.Method.invoke(Method.java:498)",
      "  at org.apache.spark.sql.catalyst.encoders.OuterScopes$.$anonfun$getOuterScope$2(OuterScopes.scala:76)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.$anonfun$doGenCode$1(objects.scala:509)",
      "  at scala.Option.map(Option.scala:230)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:509)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)",
      "  at org.apache.spark.sql.catalyst.expressions.If.doGenCode(conditionalExpressions.scala:71)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.$anonfun$prepareArguments$3(objects.scala:96)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.immutable.List.map(List.scala:298)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.prepareArguments(objects.scala:95)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.prepareArguments$(objects.scala:63)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.prepareArguments(objects.scala:456)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:507)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)",
      "  at org.apache.spark.sql.catalyst.expressions.If.doGenCode(conditionalExpressions.scala:71)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.$anonfun$prepareArguments$3(objects.scala:96)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.prepareArguments(objects.scala:95)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike.prepareArguments$(objects.scala:63)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.prepareArguments(objects.scala:456)",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:507)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)",
      "  at org.apache.spark.sql.execution.DeserializeToObjectExec.doConsume(objects.scala:91)",
      "  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)",
      "  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)",
      "  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)",
      "  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)",
      "  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)",
      "  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)",
      "  at org.apache.spark.sql.execution.DeserializeToObjectExec.doProduce(objects.scala:87)",
      "  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.DeserializeToObjectExec.produce(objects.scala:75)",
      "  at org.apache.spark.sql.execution.MapElementsExec.doProduce(objects.scala:275)",
      "  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.MapElementsExec.produce(objects.scala:264)",
      "  at org.apache.spark.sql.execution.SerializeFromObjectExec.doProduce(objects.scala:121)",
      "  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)",
      "  at org.apache.spark.sql.execution.SerializeFromObjectExec.produce(objects.scala:108)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)",
      "  at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)",
      "  at datasetAPI(<console>:54)",
      "  ... 33 elided",
      ""
     ]
    }
   ],
   "source": [
    "sealed case class SensorData(sensor_id: Int, temperature: Int, humidity: Int, timestamp: Long)\n",
    "\n",
    "  def datasetAPI(spark: SparkSession) = {\n",
    "    import spark.implicits._\n",
    "    import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "    val schema = ScalaReflection.schemaFor[SensorData].dataType.asInstanceOf[StructType]\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .schema(schema)\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/data1.json\")\n",
    "      .as[SensorData]\n",
    "      .map { sensor =>\n",
    "        val roundTimestamp = (sensor.timestamp / 60).toInt * 60\n",
    "        (sensor.copy(timestamp = roundTimestamp), 1)\n",
    "      }\n",
    "      .groupByKey { case (sensor, _) =>\n",
    "        (sensor.sensor_id, sensor.timestamp)\n",
    "      }\n",
    "      .reduceGroups { (sensor1, sensor2) =>\n",
    "          (\n",
    "            sensor1._1.copy(\n",
    "              temperature = sensor1._1.temperature + sensor2._1.temperature,\n",
    "              humidity = sensor1._1.humidity + sensor2._1.humidity,\n",
    "            ),\n",
    "            sensor1._2 + sensor2._2\n",
    "          )\n",
    "      }\n",
    "      .map { case (_, (sensor, count)) =>\n",
    "        sensor.copy(\n",
    "          temperature = sensor.temperature / count,\n",
    "          humidity = sensor.humidity / count\n",
    "        )\n",
    "      }\n",
    "      .sort(\"sensor_id\", \"timestamp\")\n",
    "      .show()\n",
    "  }\n",
    "\n",
    "datasetAPI(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ef7f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(total_messages)|\n",
      "+-------------------+\n",
      "|  11.11111111111111|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extra1: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//1. Calcular el numero de mensajes medio que reportan entre todos los sensores por minuto.\n",
    "def extra1(spark: SparkSession): Unit = {\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(s\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .groupBy($\"sensor_id\", window($\"timestamp\".cast(TimestampType), \"1 minute\"))\n",
    "      .agg(count(\"*\").as(\"total_messages\"))\n",
    "      .agg(avg(\"total_messages\"))\n",
    "      .show()\n",
    "  }\n",
    "\n",
    "extra1(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0768e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------------+-------------------+\n",
      "|sensor_id|       temperature|         humidity|          timestamp|\n",
      "+---------+------------------+-----------------+-------------------+\n",
      "|        1|          28.28125|         64.40625|2020-09-10 19:00:00|\n",
      "|        2| 28.58823529411765|65.91176470588235|2020-09-10 19:00:00|\n",
      "|        3|28.705882352941178|67.73529411764706|2020-09-10 19:00:00|\n",
      "+---------+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extra2: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//2. Calcular la temperatura y humedad media por sensor en cada hora.\n",
    "  def extra2(spark: SparkSession): Unit = {\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .groupBy($\"sensor_id\", window($\"timestamp\".cast(TimestampType), \"1 hour\"))\n",
    "      .agg(avg($\"temperature\").as(\"temperature\"), avg($\"humidity\").as(\"humidity\"))\n",
    "      .withColumn(\"timestamp\", $\"window.start\")\n",
    "      .drop($\"window\")\n",
    "      .sort(\"sensor_id\", \"timestamp\")\n",
    "      .show()\n",
    "  }\n",
    "\n",
    "extra2(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8650b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------+\n",
      "|       temperature|         humidity| timestamp|\n",
      "+------------------+-----------------+----------+\n",
      "| 28.63888888888889|64.58333333333333|1599758520|\n",
      "|28.580645161290324|67.16129032258064|1599758580|\n",
      "|28.363636363636363|66.60606060606061|1599758640|\n",
      "+------------------+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extra3: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//3. Calcular la temperatura y humedad media por minuto de todos los sensores.\n",
    "  def extra3(spark: SparkSession): Unit = {\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .groupBy(window($\"timestamp\".cast(TimestampType), \"1 minute\"))\n",
    "      .agg(avg($\"temperature\").as(\"temperature\"), avg($\"humidity\").as(\"humidity\"))\n",
    "      .withColumn(\"timestamp\", $\"window.start\".cast(LongType))\n",
    "      .drop($\"window\")\n",
    "      .sort(\"timestamp\")\n",
    "      .show()\n",
    "  }\n",
    "\n",
    "extra3(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867f4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------------------+\n",
      "|sensor_id|      temperature|          timestamp|\n",
      "+---------+-----------------+-------------------+\n",
      "|        3|27.77777777777778|2020-09-10 19:22:00|\n",
      "|        1|             27.9|2020-09-10 19:23:00|\n",
      "|        2|28.11111111111111|2020-09-10 19:24:00|\n",
      "+---------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extra4: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//4. Calcular el sensor con la temperatura más baja de cada minuto.\n",
    "  def extra4(spark: SparkSession): Unit = {\n",
    "\n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"Data/exercise5_sparkcore_data/*.json\")\n",
    "      .select($\"sensor_id\", $\"timestamp\", $\"temperature\")\n",
    "      .groupBy($\"sensor_id\", window($\"timestamp\".cast(TimestampType), \"1 minute\"))\n",
    "      .agg(avg($\"temperature\").as(\"temperature\"))\n",
    "      .withColumn(\"timestamp\", $\"window.start\")\n",
    "      .drop($\"window\")\n",
    "      .withColumn(\"row\", row_number.over(Window.partitionBy(\"timestamp\").orderBy($\"temperature\".asc)))\n",
    "      .where($\"row\" === 1)\n",
    "      .drop($\"row\")\n",
    "      .sort($\"timestamp\".asc)\n",
    "      .show()\n",
    "  }\n",
    "\n",
    "extra4(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cd09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
