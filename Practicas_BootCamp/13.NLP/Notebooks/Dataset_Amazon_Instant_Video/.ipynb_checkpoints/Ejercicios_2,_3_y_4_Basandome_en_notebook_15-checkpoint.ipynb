{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83202f5c",
   "metadata": {},
   "source": [
    "# Ejercicio 2 - Etapa de preprocesado de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3542478",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0cd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "import sklearn.preprocessing as pr\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRUV2, SimpleRNN\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98a51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesado(path, size):\n",
    "    \n",
    "    df = pd.read_json(path, lines=True, \n",
    "                        compression='gzip')[:size][['reviewText', 'overall']]\n",
    "    \n",
    "    df.overall = [1 if int(row) > 2 else 0 for row in df.overall] \n",
    "    \n",
    "    #Balanceo de etiquetas\n",
    "    label_1, label_0 = df['overall'].value_counts()\n",
    "\n",
    "    df = pd.concat([df[df.overall == 1].sample(label_0 * 2),\n",
    "                    df[df.overall == 0]],\n",
    "                   axis=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.reviewText,\n",
    "        df.overall,   \n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    data_train = [sentence for sentence in X_train.values]\n",
    "    labels_train = [label for label in y_train.values]\n",
    "    data_test = [sentence for sentence in X_test.values]\n",
    "    labels_test = [label for label in y_test.values]      \n",
    "    \n",
    "    def review_to_words(review):\n",
    "        \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "        text = BeautifulSoup(review, \"html5lib\").get_text()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", review.lower())\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "        words = [PorterStemmer().stem(w) for w in words]    \n",
    "        return words\n",
    "    \n",
    "    words_train = list(map(review_to_words, data_train))\n",
    "    words_test = list(map(review_to_words, data_test)) \n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=5000,\n",
    "             preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "\n",
    "    features_train_gradient = vectorizer.fit_transform(words_train).toarray()\n",
    "    features_test_gradient = vectorizer.transform(words_test).toarray()\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    for sentence in words_train:\n",
    "        words = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                words.append(vocabulary[word])\n",
    "            except:\n",
    "                pass\n",
    "        features_train.append(words)\n",
    "    \n",
    "    for sentence in words_test:\n",
    "        words = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                words.append(vocabulary[word])\n",
    "            except:\n",
    "                pass\n",
    "        features_test.append(words)\n",
    "        \n",
    "    features_train = sequence.pad_sequences(features_train, maxlen=500)    \n",
    "    features_test = sequence.pad_sequences(features_test, maxlen=500)\n",
    "    \n",
    "    return features_train,\\\n",
    "           features_test,\\\n",
    "           np.array(labels_train),\\\n",
    "           np.array(labels_test),\\\n",
    "           vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c923a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-14 05:10:08--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\n",
      "Resolviendo snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Conectando con snap.stanford.edu (snap.stanford.edu)[171.64.75.80]:80... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 9517526 (9,1M) [application/x-gzip]\n",
      "Guardando como: “reviews_Amazon_Instant_Video_5.json.gz”\n",
      "\n",
      "reviews_Amazon_Inst 100%[===================>]   9,08M  1,95MB/s    en 7,9s    \n",
      "\n",
      "2022-02-14 05:10:17 (1,15 MB/s) - “reviews_Amazon_Instant_Video_5.json.gz” guardado [9517526/9517526]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\n",
    "features_train,\\\n",
    "features_test,\\\n",
    "labels_train,\\\n",
    "labels_test,\\\n",
    "vocabulary = preprocesado('reviews_Amazon_Instant_Video_5.json.gz',37126)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ad529",
   "metadata": {},
   "source": [
    "# Ejercicio 3 -  Etapa de entrenamiento y testeo de un modelo de análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291e77f",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1ec276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_model(mod, emb_size, vocabulary_size, max_words):\n",
    "    embedding_size = emb_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_size, input_length = max_words))\n",
    "    model.add(mod(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def entreno(batch_size, num_epochs, X_train, labels_train, model):\n",
    "    \n",
    "    X_valid, y_valid = X_train[:batch_size], labels_train[:batch_size]  # first batch_size samples\n",
    "    X_train2, y_train2 = X_train[batch_size:], labels_train[batch_size:]  # rest for training\n",
    "\n",
    "    model.fit(X_train2, y_train2,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              batch_size=batch_size, epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def evaluacion(model, X_test, labels_test):        \n",
    "    print(\"Test accuracy:\", model.evaluate(X_test, labels_test, verbose=0)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28db2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "157/157 [==============================] - 28s 172ms/step - loss: 0.5621 - accuracy: 0.7047 - val_loss: 0.4133 - val_accuracy: 0.8750\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.7941720485687256\n",
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               40200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 200,301\n",
      "Trainable params: 200,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "157/157 [==============================] - 30s 185ms/step - loss: 0.6469 - accuracy: 0.6540 - val_loss: 0.4692 - val_accuracy: 0.7500\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.7668825387954712\n",
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "157/157 [==============================] - 13s 79ms/step - loss: 0.6976 - accuracy: 0.5124 - val_loss: 0.6961 - val_accuracy: 0.4688\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.5499537587165833\n"
     ]
    }
   ],
   "source": [
    "for mod in [LSTM, GRUV2, SimpleRNN]:\n",
    "    print(\"\\nNuevo modelo\\n\")\n",
    "    model = crear_model(mod, 32, len(vocabulary), 500)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"\\nComienza entrenamiento\\n\")\n",
    "    entreno(32, 1, features_train, labels_train, model)\n",
    "    print(\"\\nComienza evaluación\\n\")\n",
    "    evaluacion(model, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fa51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
