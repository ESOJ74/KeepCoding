{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RegressionUsingNumericalData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4-fbTTeuvkC"
      },
      "source": [
        "You can use this exercise as a guide to predict house prices using numerical in a real dataset. It is based on this tutorial https://www.pyimagesearch.com/2019/01/21/regression-with-keras/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCvuioWzuoPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0a6b23-8142-447a-ee43-c39fdf0926d5"
      },
      "source": [
        "# Downloading the data\n",
        "!git clone https://github.com/emanhamed/Houses-dataset\n",
        "!ls -la Houses-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2165\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 37.70 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "total 100\n",
            "drwxr-xr-x 4 root root  4096 Jan 14 20:34  .\n",
            "drwxr-xr-x 1 root root  4096 Jan 14 20:33  ..\n",
            "drwxr-xr-x 8 root root  4096 Jan 14 20:34  .git\n",
            "-rw-r--r-- 1 root root   378 Jan 14 20:34  .gitattributes\n",
            "-rw-r--r-- 1 root root  1726 Jan 14 20:34  .gitignore\n",
            "drwxr-xr-x 2 root root 73728 Jan 14 20:34 'Houses Dataset'\n",
            "-rw-r--r-- 1 root root  1767 Jan 14 20:34  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWzhspe9l44C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9af9ca-99c2-43e6-c684-ade7e8bd4fae"
      },
      "source": [
        "# Showing some lines in the txt file\n",
        "!head -n 10 Houses-dataset/Houses\\ Dataset/HousesInfo.txt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 4 4053 85255 869500\n",
            "4 3 3343 36372 865200\n",
            "3 4 3923 85266 889000\n",
            "5 5 4022 85262 910000\n",
            "3 4 4116 85266 971226\n",
            "4 5 4581 85266 1249000\n",
            "3 4 2544 85262 799000\n",
            "4 5 5524 85266 1698000\n",
            "3 4 4229 85255 1749000\n",
            "4 5 3550 85262 1500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzA1Nw96uiGD"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_house_attributes(inputPath):\n",
        "\n",
        "\t# initialize the list of column names in the CSV file and then load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        " \n",
        "  # determine (1) the unique zip codes and (2) the number of data points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        " \n",
        "\t# loop over each of the unique zip codes and their corresponding count\n",
        "\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "   \n",
        "\t\t# the zip code counts for our housing dataset is *extremely* unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "   \n",
        "\t# return the data frame\n",
        "\treturn df\n",
        "\n",
        "def process_house_attributes(df, train, test):\n",
        "\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "  \n",
        "\t# performin min-max scaling each continuous feature column to the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
        "\ttestContinuous = cs.transform(test[continuous])\n",
        " \n",
        "  # one-hot encode the zip code categorical data (by definition of one-hot encoing, all output features are now in the range [0, 1])\n",
        "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        " \n",
        "\t# construct our training and testing data points by concatenating the categorical features with the continuous features\n",
        "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
        "\ttestX = np.hstack([testCategorical, testContinuous])\n",
        " \n",
        "\t# return the concatenated training and testing data\n",
        "\treturn (trainX, testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RJiH1RygVvX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOHLCkHxvqrB"
      },
      "source": [
        "Let's define the model as in the tutorial:\n",
        "\n",
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_arch.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SkCFGfmvvTK"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input\n",
        "\n",
        "\n",
        "def create_mlp(dim, regress=False):\n",
        "\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(16, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(8, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        " \n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "  \n",
        "\t# return our model\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G843f_MwAMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df72d7ba-e9f3-4eaf-8400-3edb6f1a3a5a"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import locale\n",
        "import os\n",
        "\n",
        "# construct the path to the input .txt file that contains information on each house in the dataset and then load the dataset\n",
        "print(\"[INFO] loading house attributes...\")\n",
        "\n",
        "# inputPath = os.path.sep.join([args[\"dataset\"], \"HousesInfo.txt\"])\n",
        "inputPath = \"Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "df = load_house_attributes(inputPath)\n",
        "\n",
        "# construct a training and testing split with 75% of the data used for training and the remaining 25% for evaluation\n",
        "print(\"[INFO] constructing training/testing split...\")\n",
        "(train, test) = train_test_split(df, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading house attributes...\n",
            "[INFO] constructing training/testing split...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxWfx8RsJtIJ",
        "outputId": "bbc112cd-6f4c-4e58-c956-f43cbfff11b6"
      },
      "source": [
        "print(train[\"price\"].max())\n",
        "print(train[\"price\"].min())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5858000\n",
            "36000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx6e4whUw3Sr"
      },
      "source": [
        "# find the largest house price in the training set and use it to scale our house prices to the range [0, 1] (this will lead to better training and convergence)\n",
        "maxPrice = train[\"price\"].max()\n",
        "trainY = train[\"price\"] / maxPrice\n",
        "testY = test[\"price\"] / maxPrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lvsDw7UN1TG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqR_uHpzJq-h",
        "outputId": "e17ba4e9-6eb2-47cb-fe10-12da5b534e41"
      },
      "source": [
        "print(trainY.max())\n",
        "print(trainY.min())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.006145442130419939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10FtqPuayWXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2736ba-dacd-4464-de20-0aec4140c0fe"
      },
      "source": [
        "# process the house attributes data by performing min-max scaling\n",
        "# on continuous features, one-hot encoding on categorical features,\n",
        "# and then finally concatenating them together\n",
        "print(\"[INFO] processing data...\")\n",
        "(trainX, testX) = process_house_attributes(df, train, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] processing data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainX.shape)\n",
        "print(testX.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5f4jCcmV66F",
        "outputId": "832930b8-ccc2-44df-9f87-44cafd738e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(271, 10)\n",
            "(91, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heJAL2CiyYYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8205b428-09b5-4d11-d564-8cd69129a87a"
      },
      "source": [
        "# create our MLP and then compile the model using mean absolute percentage error as our loss, implying that we seek to minimize\n",
        "# the absolute percentage difference between our price *predictions* and the *actual prices*\n",
        "\n",
        "model = create_mlp(trainX.shape[1], regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(x=trainX, y=trainY, \n",
        "\t        validation_data=(testX, testY),\n",
        "\t        epochs=200, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 6ms/step - loss: 104.4267 - val_loss: 82.5521\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 60.2591 - val_loss: 48.1079\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 42.0330 - val_loss: 31.2145\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 31.0133 - val_loss: 28.7841\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 28.8872 - val_loss: 28.3920\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 30.0653 - val_loss: 27.9217\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 28.3277 - val_loss: 29.8467\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 32.4921 - val_loss: 30.7374\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 32.2018 - val_loss: 25.8013\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 28.4066 - val_loss: 26.7982\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 26.0875 - val_loss: 26.7162\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 25.8930 - val_loss: 25.8708\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 27.5752 - val_loss: 26.4896\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 27.4286 - val_loss: 29.2726\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 26.3791 - val_loss: 25.2955\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 24.6440 - val_loss: 25.3824\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 25.0361 - val_loss: 24.3189\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 25.4448 - val_loss: 24.5201\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 24.4040 - val_loss: 37.6520\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 32.7617 - val_loss: 32.7719\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 27.9745 - val_loss: 25.5737\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 24.1067 - val_loss: 22.9896\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 24.0421 - val_loss: 22.2719\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 25.7608 - val_loss: 24.3184\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.5944 - val_loss: 24.0492\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 21.9039 - val_loss: 24.3656\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.3054 - val_loss: 24.5797\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.3002 - val_loss: 24.6741\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 21.9848 - val_loss: 23.3439\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 24.1127 - val_loss: 25.5451\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.2264 - val_loss: 22.8405\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.8771 - val_loss: 25.3500\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.9005 - val_loss: 24.6932\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.2529 - val_loss: 26.8495\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 24.9731 - val_loss: 23.2774\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.4828 - val_loss: 24.5537\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.2035 - val_loss: 25.7079\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.6081 - val_loss: 23.5322\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.9475 - val_loss: 25.4805\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.8263 - val_loss: 25.2769\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.0432 - val_loss: 26.3090\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.1884 - val_loss: 21.7266\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.8795 - val_loss: 21.0704\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.4952 - val_loss: 24.9657\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 22.8567 - val_loss: 23.2336\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.4657 - val_loss: 27.0024\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 22.7768 - val_loss: 23.6699\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.0106 - val_loss: 23.7502\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.4364 - val_loss: 24.0791\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.5867 - val_loss: 22.0434\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.2674 - val_loss: 22.0468\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.5820 - val_loss: 24.8437\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.5394 - val_loss: 24.2341\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 22.1371 - val_loss: 22.3983\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.7869 - val_loss: 21.6695\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 21.0340 - val_loss: 24.7626\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.8078 - val_loss: 23.7367\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.1275 - val_loss: 23.1778\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 20.4550 - val_loss: 22.6061\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.0757 - val_loss: 21.8644\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.0750 - val_loss: 24.7017\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.7281 - val_loss: 23.2031\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.5181 - val_loss: 22.3453\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.0256 - val_loss: 21.7237\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.2122 - val_loss: 27.0359\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.1248 - val_loss: 23.1522\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.6297 - val_loss: 22.1225\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.0840 - val_loss: 24.2580\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.8019 - val_loss: 26.8118\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.6167 - val_loss: 25.5077\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.2865 - val_loss: 22.5616\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.9618 - val_loss: 27.0600\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.6021 - val_loss: 22.3079\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.0179 - val_loss: 22.8492\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 23.3861 - val_loss: 25.5022\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.3728 - val_loss: 23.6926\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.5312 - val_loss: 22.9219\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.2246 - val_loss: 22.9729\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8156 - val_loss: 20.6388\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8851 - val_loss: 22.5384\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.6579 - val_loss: 26.5098\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.2130 - val_loss: 22.0635\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.5602 - val_loss: 22.7311\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.0523 - val_loss: 23.1418\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.6993 - val_loss: 24.5311\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5900 - val_loss: 21.5949\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.7106 - val_loss: 22.5277\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.3774 - val_loss: 24.0317\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.2873 - val_loss: 23.8396\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.8139 - val_loss: 22.4058\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.4189 - val_loss: 21.4441\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 22.0754 - val_loss: 24.6982\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.3471 - val_loss: 20.9425\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.0625 - val_loss: 28.4359\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5090 - val_loss: 22.2561\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.3813 - val_loss: 22.2269\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.7795 - val_loss: 22.5771\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.3624 - val_loss: 22.5877\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.2763 - val_loss: 23.7219\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.0552 - val_loss: 23.8497\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.3525 - val_loss: 25.0876\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4997 - val_loss: 23.1001\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 18.6149 - val_loss: 25.1662\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.3254 - val_loss: 21.1625\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.4675 - val_loss: 24.3542\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8738 - val_loss: 21.4233\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.0918 - val_loss: 21.3273\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.3424 - val_loss: 24.6901\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.1451 - val_loss: 23.9397\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.0446 - val_loss: 22.8303\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8978 - val_loss: 21.2885\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.2033 - val_loss: 22.7091\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4876 - val_loss: 20.0108\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4437 - val_loss: 26.8690\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.5312 - val_loss: 26.5578\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 24.2620 - val_loss: 25.5237\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.9216 - val_loss: 24.4842\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.1911 - val_loss: 26.5324\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.3063 - val_loss: 22.3047\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.3685 - val_loss: 25.9138\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.3899 - val_loss: 26.6608\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.7341 - val_loss: 22.3457\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.9184 - val_loss: 24.0778\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.5429 - val_loss: 24.2005\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.9131 - val_loss: 21.1460\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.4142 - val_loss: 22.5262\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.4335 - val_loss: 23.3367\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.1264 - val_loss: 20.8338\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.0814 - val_loss: 22.4975\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.6088 - val_loss: 26.4601\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5268 - val_loss: 27.3075\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4236 - val_loss: 22.2183\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5303 - val_loss: 23.8415\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.8553 - val_loss: 22.6439\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.8075 - val_loss: 22.7865\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.4465 - val_loss: 23.0417\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.9205 - val_loss: 22.8752\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.7803 - val_loss: 22.9647\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 18.6768 - val_loss: 24.6908\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.5790 - val_loss: 25.1587\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4927 - val_loss: 24.4105\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.6836 - val_loss: 22.2794\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.8059 - val_loss: 23.2025\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8305 - val_loss: 27.9771\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5652 - val_loss: 22.1587\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.5224 - val_loss: 24.8012\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.0752 - val_loss: 21.5746\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4259 - val_loss: 21.6108\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 23.3957 - val_loss: 22.7790\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 20.5097 - val_loss: 22.3185\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.7435 - val_loss: 21.2163\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.6632 - val_loss: 22.2557\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4455 - val_loss: 22.5231\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.1808 - val_loss: 22.2940\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.7102 - val_loss: 24.1494\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.4328 - val_loss: 22.0627\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.2299 - val_loss: 22.5153\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.1034 - val_loss: 22.1226\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5605 - val_loss: 24.7064\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.1367 - val_loss: 29.3608\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.3950 - val_loss: 21.7891\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.8876 - val_loss: 22.4281\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 18.9263 - val_loss: 23.0555\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.7526 - val_loss: 27.9926\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.4219 - val_loss: 23.3505\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.7339 - val_loss: 22.7089\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.4248 - val_loss: 22.9650\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.5378 - val_loss: 25.8364\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 23.5734 - val_loss: 23.7343\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.5899 - val_loss: 23.7806\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.5975 - val_loss: 22.2831\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.2878 - val_loss: 21.5968\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.0350 - val_loss: 24.2665\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.4324 - val_loss: 22.5546\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.1284 - val_loss: 22.4331\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.1274 - val_loss: 24.0985\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.1021 - val_loss: 21.8639\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.1883 - val_loss: 22.2553\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 22.7454 - val_loss: 24.9260\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 21.9674 - val_loss: 25.7032\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 19.8477 - val_loss: 21.1636\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.5725 - val_loss: 23.3633\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.0220 - val_loss: 25.2188\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.7107 - val_loss: 20.6013\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.7563 - val_loss: 23.7470\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.3777 - val_loss: 22.4512\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 22.0052 - val_loss: 22.1152\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.4671 - val_loss: 22.0746\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.4507 - val_loss: 22.4002\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 20.0092 - val_loss: 24.2157\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 20.3741 - val_loss: 23.8120\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.6202 - val_loss: 25.1361\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.9082 - val_loss: 21.2089\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 17.8672 - val_loss: 24.0594\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.7085 - val_loss: 20.4092\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.0157 - val_loss: 21.0925\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 19.3144 - val_loss: 23.6405\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 18.4108 - val_loss: 23.1002\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.5844 - val_loss: 22.6979\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 3ms/step - loss: 18.6626 - val_loss: 24.0822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb08ee7dc10>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up9nKR3Dyaka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b24ff6c-bc0c-479f-ec2e-7466b2ba1290"
      },
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(testX)\n",
        "\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# compute the mean and standard deviation of the absolute percentage difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] predicting house prices...\n",
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 27.84%, std: 27.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvv0WM6Z6DLN"
      },
      "source": [
        "Here it is the example that combines numerical data and images https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/.\n"
      ]
    }
  ]
}